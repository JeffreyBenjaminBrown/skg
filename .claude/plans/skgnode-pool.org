* Context

Profiling shows ~11s save-buffer latency, mostly TypeDB queries.
The previous plan (now =typedb-batching_obsoleting.org=) addressed this
by batching queries. This plan goes further: keep an in-memory pool of
SkgNodes across requests, plus a map from each view URI to the PIDs
it displays.

This nearly eliminates TypeDB and disk lookups for already-displayed
nodes. The save path benefits most: after the initial view, almost every
node is already in the pool.

Additionally, saving must update *all* views that contain changed nodes,
not just the buffer the user saved.

Batch query optimizations (=delete_out_links=, =insert_relationship_from_list=,
=pids_and_sources_from_ids=) are deferred to a follow-up
(see .claude/plans/typedb-batching.org).
They remain useful for cache misses and for the write side.


* Prerequisites: view URIs

The server currently has no way to identify which buffer a request came
from. We need per-view identifiers.

Emacs generates a UUID (via =(org-id-uuid)=) when creating a view
buffer, stores it as a buffer-local variable =skg-view-uri=, and sends
it with every request. The identifier is opaque to the server. Two views
of the same node get different URIs. If a buffer is killed and recreated
for the same node, it gets a new URI.

To survive major mode changes (=kill-all-local-variables=), mark the
variable as permanent:

#+begin_src elisp
(defvar-local skg-view-uri nil
  "Unique view URI for this skg buffer.")
(put 'skg-view-uri 'permanent-local t)
#+end_src


* Step 1: =ViewUri= type and protocol changes

** Server: =ViewUri= newtype

Add to =server/types/viewnode.rs=:

#+begin_src rust
#[derive(Clone, Debug, PartialEq, Eq, Hash)]
pub struct ViewUri ( pub String );
#+end_src

** Protocol changes

View request becomes:
: ((request . "single root content view") (id . "NODE_ID") (view-uri . "UUID"))

Save request becomes:
: ((request . "save buffer") (view-uri . "UUID"))

New close-view message:
: ((request . "close view") (view-uri . "UUID"))

=view-uri= is required in all requests.

Save response becomes (to support multi-view updates):
: ((content "...") (errors (...)) (updated-views (("URI1" "content1") ("URI2" "content2") ...)))

The =updated-views= list contains re-rendered content for every OTHER
view that was affected by the save. The =content= field is the saved
buffer's own re-rendered content (as before).

** Emacs changes

=elisp/skg-buffer.el=:
- Add =defvar-local skg-view-uri nil= with =permanent-local= property.
- In =skg-open-org-buffer-from-text=, set =skg-view-uri= to a UUID.
- Add buffer-local =kill-buffer-hook= that sends the close-view message.

=elisp/skg-request-single-root-content-view.el=:
- Generate UUID *before* sending request (the buffer does not exist yet).
- Include =(view-uri . UUID)= in request.
- Pass UUID into the response handler via closure capture so it can
  set =skg-view-uri= on the new buffer.

=elisp/skg-request-save.el=:
- Include =(view-uri . skg-view-uri)= in the request line.
- Response handler: after updating the saved buffer, process
  =updated-views=: for each =(uri content)= pair, find the buffer
  whose =skg-view-uri= matches and replace its content.

** Unsaved-modifications guard

If any skg buffer has unsaved modifications and the user tries to edit
a different skg buffer, show a warning. Implement via =first-change-hook=
in each skg buffer:

#+begin_src elisp
(defun skg-warn-if-other-buffer-modified ()
  "Warn if another skg buffer has unsaved modifications."
  (let ((other-modified
         (cl-some (lambda (buf)
                    (and (not (eq buf (current-buffer)))
                         (buffer-local-value 'skg-view-uri buf)
                         (buffer-modified-p buf)))
                  (buffer-list))))
    (when other-modified
      (message "WARNING: Another skg buffer has unsaved modifications."))))
#+end_src

Add to =first-change-hook= (buffer-local) when creating skg buffers.

** Buffer-modified indicator

Skg buffers already call =(set-buffer-modified-p nil)= after receiving
server content (=skg-replace-buffer-with-new-content= at line 79 of
=skg-request-save.el=). When the user edits, Emacs automatically sets
=buffer-modified-p= to =t=, so the =**= mode-line indicator should
already work. Verify during testing; if not, add
=(set-buffer-modified-p t)= in a =first-change-hook=.

** Finding buffers by URI

When processing =updated-views=, Emacs must find the buffer whose
=skg-view-uri= matches a given URI:

#+begin_src elisp
(defun skg-find-buffer-by-uri (uri)
  (cl-find-if (lambda (buf)
                (string= uri (buffer-local-value 'skg-view-uri buf)))
              (buffer-list)))
#+end_src

** Files to modify

- =server/types/viewnode.rs=
- =server/serve/util.rs= (add helper to extract =view-uri= from sexp)
- =elisp/skg-buffer.el=
- =elisp/skg-request-single-root-content-view.el=
- =elisp/skg-request-save.el=


* Step 2: =SkgnodesInMemory= data type

** File

New file: =server/types/skg_memory.rs=.
Add =pub mod skg_memory;= to =server/types/mod.rs=.

** Per-view state

The server needs to know each view's root IDs (for re-rendering) and
current PID set (for determining which views are affected by a save).

#+begin_src rust
pub struct ViewState {
  pub root_ids : Vec<ID>,
  pub pids     : HashSet<ID>,
}
#+end_src

When the user adds or removes a top-level headline and saves, the
forest's root children change. After =complete_viewtree=, we extract
the new root IDs from the forest (children of BufferRoot that are
TrueNodes) and pass them to =update_view=. This keeps
=ViewState.root_ids= current so that re-rendering from another view's
save uses the correct roots.

** Struct

#+begin_src rust
/// Persistent cross-request cache of all SkgNodes currently displayed
/// in any Emacs view. The pool is the authoritative in-memory store;
/// SkgNodeMap is the per-request transactional layer that shadows it.
/// See also: SkgNodeMap (the per-request working set).
pub struct SkgnodesInMemory {
  pool         : HashMap<ID, SkgNode>,
  // The reverse lookup (PID -> views) is computed by scanning `views`
  // via views_containing(). This is O(views) per call, fine for < 10
  // views. If the number of views grows large, consider a bijective
  // map (HashMap<ID, HashSet<ViewUri>> maintained alongside this one)
  // for O(1) reverse lookups.
  views        : HashMap<ViewUri, ViewState>,
  id_resolver  : HashMap<ID, (ID, SourceName)>,
  // DEFERRED OPTIMIZATION: id_resolver is not yet wired into the render
  // pipeline (would require threading &mut SkgnodesInMemory through
  // render_initial_forest_bfs and complete_viewtree). If pid_and_source_from_id
  // remains a bottleneck after pool seeding, wire id_resolver into
  // skgnode_and_viewnode_from_id, or apply the batch-query optimization
  // from typedb-batching.org.
}
#+end_src

- =pool=: all SkgNodes currently displayed in any view, keyed by PID.
- =views=: per-view state (root IDs + current PID set).
- =id_resolver=: maps any ID (PID or extra_id) to (PID, source).
  Avoids repeated =pid_and_source_from_id= calls.

** Methods

#+begin_src rust
fn new              ()                                 -> Self
fn get_from_pool    (&self, pid: &ID)                  -> Option<&SkgNode>
fn upsert_in_pool   (&mut self, pid: ID, node: SkgNode)
fn resolve_id       (&self, id: &ID)                   -> Option<&(ID, SourceName)>
fn cache_id_resolution(&mut self, id: ID, pid: ID, source: SourceName)
fn pids_in_view     (&self, uri: &ViewUri)             -> Vec<ID>
fn root_ids_in_view (&self, uri: &ViewUri)             -> Option<&[ID]>
fn register_view    (&mut self, uri: ViewUri, root_ids: Vec<ID>, pids: &[ID])
fn update_view      (&mut self, uri: &ViewUri, new_root_ids: Vec<ID>, new_pids: &[ID])
fn unregister_view  (&mut self, uri: &ViewUri)
fn views_containing (&self, pid: &ID)                  -> Vec<ViewUri>
fn gc               (&mut self)
#+end_src

** How =update_view= works

=update_view(uri, new_root_ids, new_pids)= replaces the view's root IDs
and entire PID set. Internally it:
1. Looks up the =ViewState= for =uri= in =views=.
2. Replaces =root_ids= with =new_root_ids= and =pids= with =new_pids=.
3. Calls =gc()=, which removes from =pool= any PID that no longer
   appears in ANY view's PID set.

The caller provides the final PID set (collected from the re-rendered
forest). Removals are implicit: any PID in the old set but not the new
set loses its association with this view; if no other view references
it, =gc= removes it from the pool.

** GC

#+begin_src rust
// SCALING NOTE: gc() is O(views * pids_per_view). For typical usage
// (< 10 views, < 1000 nodes each) this is negligible. If it becomes
// a bottleneck, replace with reference counting: increment on
// register/update, decrement on update/unregister, remove at zero.
fn gc (&mut self) {
  let referenced : HashSet<ID> =
    self . views . values ()
    . flat_map ( |vs| vs . pids . iter () . cloned () )
    . collect ();
  self . pool . retain ( |pid, _| referenced . contains ( pid ));
  self . id_resolver . retain (
    |_, (target_pid, _)| referenced . contains ( target_pid )); }
#+end_src

Called internally by =update_view= and =unregister_view=.


* Step 3: wire into =ConnectionState=

** File: =server/serve.rs=

Expand =ConnectionState= (line 24):

#+begin_src rust
pub struct ConnectionState {
  pub diff_mode_enabled : bool,
  pub memory            : SkgnodesInMemory,
  // PITFALL: If Emacs crashes or the TCP connection drops without
  // sending close-view messages, SkgnodesInMemory is still freed
  // because ConnectionState is owned by handle_emacs and dropped
  // when the connection loop exits (line 96, n == 0). No leak.
  // However, the pool may briefly hold stale entries for views
  // that were conceptually "closed" by the crash. This is harmless:
  // the entries are freed moments later when ConnectionState drops.
}
#+end_src

Initialize at line 83:
#+begin_src rust
let mut conn_state : ConnectionState =
  ConnectionState {
    diff_mode_enabled : false,
    memory            : SkgnodesInMemory::new (),
  };
#+end_src

** Threading =&mut conn_state= to handlers

| Handler                           | Currently receives                | Changes to                                          |
|-----------------------------------+-----------------------------------+-----------------------------------------------------|
| =handle_single_root_view_request= | =diff_mode_enabled : bool=        | =conn_state : &mut ConnectionState=                 |
| =handle_save_buffer_request=      | =diff_mode_enabled : bool=        | =request : &str, conn_state : &mut ConnectionState= |
| =handle_git_diff_mode_request=    | =conn_state : &mut ...= (already) | no change                                           |

New handler: =handle_close_view_request(stream, request, conn_state)=.
Add dispatch branch at ~line 133 of =serve.rs=.


* Step 4: view creation path

** =single_root_view.rs= (=server/serve/handlers/=)

- Accept =conn_state : &mut ConnectionState=.
- Extract =view-uri= from request.
- Call =multi_root_view=.
- After success, register the view in the pool.

** =content_view.rs= (=server/to_org/render/=)

=multi_root_view= currently returns =Result<String, ...>=. Change to
=Result<(String, SkgNodeMap, Vec<ID>), ...>=. Collect PIDs from the
forest inside =multi_root_view= before converting to string, then
return them alongside.

** Handler logic after rendering

#+begin_src rust
// multi_root_view returned (buffer_text, map, pids)
for (pid, skgnode) in map {
  conn_state . memory . upsert_in_pool ( pid, skgnode ); }
conn_state . memory . register_view (
  view_uri,
  vec! [ root_id . clone () ],
  &pids );
#+end_src

** Files to modify

- =server/serve/handlers/single_root_view.rs=
- =server/to_org/render/content_view.rs=


* Step 5: save buffer path

** =save_buffer.rs= (=server/serve/handlers/=)

- Accept =request : &str= and =conn_state : &mut ConnectionState=.
- Extract =view-uri= from the request line.

** Seed =SkgNodeMap= from pool

After =skgnode_map_from_save_instructions= (line 186-187), and before
=complete_viewtree=, seed the map with pool entries:

#+begin_src rust
for pid in conn_state . memory . pids_in_view ( &view_uri ) {
  if let Some ( skgnode ) = conn_state . memory . get_from_pool ( &pid ) {
    skgnode_map . entry ( pid )
      . or_insert_with ( || skgnode . clone () ); } }
#+end_src

=or_insert_with= ensures save-instruction entries (the user's edits)
take precedence over cached pool entries.

** Update pool and re-render affected views

=update_from_and_rerender_buffer= currently returns =Result<SaveResponse, ...>=.
Change to =Result<(SaveResponse, SkgNodeMap), ...>= to return the final map.

After the pipeline completes:

1. Collect PIDs from the re-rendered forest using =collect_ids_from_subtree=.
   (We use the forest, not =map.keys()=, because the map is populated
   lazily and might miss nodes that were skipped during preorder
   traversal — e.g., phantom nodes whose =is_phantom()= returns true
   cause early returns in =complete_parent_first/truenode.rs:88=, so
   their SkgNode is never looked up and never enters the map.)
2. Update pool from the final map.
3. Determine which PIDs changed (from save_instructions).
4. Collect affected views into a =HashSet<ViewUri>= (to deduplicate
   when a save changes multiple PIDs that appear in the same view).
5. Re-render each affected view.
6. Update those views in the pool too.
7. Send all re-rendered content in the response.

#+begin_src rust
// 1-2: update saved view
let new_pids : Vec<ID> =
  collect_ids_from_subtree ( &forest, forest . root () . id () );
let new_root_ids : Vec<ID> = forest . root () . children ()
  . filter_map ( |c| match &c . value () . kind {
    ViewNodeKind::True ( t ) => Some ( t . id . clone () ),
    _ => None } )
  . collect ();
for (pid, skgnode) in final_map {
  conn_state . memory . upsert_in_pool ( pid, skgnode ); }
conn_state . memory . update_view ( &view_uri, new_root_ids, &new_pids );

// 3: determine changed PIDs
let changed_pids : HashSet<ID> = save_instructions . iter ()
  . filter_map ( |instr| match instr {
    DefineNode::Save(SaveNode(n)) => n . ids . first () . cloned (),
    DefineNode::Delete(pid) => Some ( pid . clone () ) } )
  . collect ();

// 4: collect affected views (deduplicated)
let affected_views : HashSet<ViewUri> = changed_pids . iter ()
  . flat_map ( |pid| conn_state . memory . views_containing ( pid ) )
  . filter ( |uri| *uri != view_uri )
  . collect ();

// 5-6: re-render affected views
let mut updated_views : Vec<(ViewUri, String)> = Vec::new ();
for uri in affected_views {
  if let Some ( root_ids ) =
    conn_state . memory . root_ids_in_view ( &uri )
  { let root_ids_owned : Vec<ID> = root_ids . to_vec ();
    let (text, map, re_pids) : (String, SkgNodeMap, Vec<ID>) =
      multi_root_view (
        driver, config, &root_ids_owned,
        conn_state . diff_mode_enabled ) . await ?;
    for (pid, skgnode) in map {
      conn_state . memory . upsert_in_pool ( pid, skgnode ); }
    conn_state . memory . update_view ( &uri, root_ids_owned, &re_pids );
    updated_views . push ( (uri, text) ); } }
#+end_src

** Response format

Include =updated_views= in the response s-exp:

#+begin_src rust
// format_buffer_response_sexp_with_updates(content, errors, updated_views)
// produces:
// ((content "...") (errors (...))
//  (updated-views (("URI1" "content1") ("URI2" "content2"))))
#+end_src

** Files to modify

- =server/serve/handlers/save_buffer.rs=
- =server/serve.rs= (pass =&request= and =&mut conn_state= to handler)
- =server/serve/util.rs= (add =format_buffer_response_sexp_with_updates=)


* Step 6: close view handler

** New file: =server/serve/handlers/close_view.rs=

#+begin_src rust
pub fn handle_close_view_request (
  stream     : &mut TcpStream,
  request    : &str,
  conn_state : &mut ConnectionState,
) {
  match view_uri_from_request ( request ) {
    Some ( uri ) => {
      conn_state . memory . unregister_view ( &uri );
      send_response ( stream, "view closed" ); },
    None =>
      send_response ( stream, "Error: missing view-uri" ); } }
#+end_src

** Files to modify

- New: =server/serve/handlers/close_view.rs=
- =server/serve/handlers/mod.rs= (add =pub mod close_view;=)
- =server/serve.rs= (add dispatch branch)


* Step 7: ID resolution cache integration (deferred)

The =id_resolver= avoids repeated =pid_and_source_from_id=
TypeDB calls. The cleanest integration point is in =skgnode_and_viewnode_from_id=
(=server/to_org/util.rs:40=), which calls =pid_and_source_from_id=.

However, threading =&mut SkgnodesInMemory= through the entire render
pipeline is highly invasive. Defer to a follow-up.

The =id_resolver= field comment in =SkgnodesInMemory= documents this.


* Why SkgNodeMap is still needed

=SkgNodeMap= is a per-request working set. =SkgnodesInMemory= is a
per-connection persistent cache. They have different lifecycles:

- During save, =skgnode_map_from_save_instructions= builds a map
  from the user's edits. These represent *intended* state that should
  not go into the pool until the save *succeeds*. If the save fails,
  the pool stays clean.
- =complete_viewtree= mutates the map (adding nodes it discovers
  via =skgnode_from_map_or_disk=). This per-request mutation needs
  isolation from the persistent pool.
- After the request succeeds, the map's contents are merged into
  the pool.

The pool *seeds* the SkgNodeMap (step 5), and the final SkgNodeMap
flows *back* into the pool. The SkgNodeMap is the transactional layer;
the pool is the persistent layer.

Document this distinction via comments at both definition sites:

In =server/types/skgnodemap.rs= near the =SkgNodeMap= typedef:
#+begin_src rust
/// Per-request working set of SkgNodes, keyed by PID.
/// Created fresh per request, seeded from SkgnodesInMemory.pool,
/// mutated during request processing (complete_viewtree adds nodes
/// via skgnode_from_map_or_disk), and merged back into the pool
/// after the request succeeds. Provides transactional isolation:
/// if the request fails, the pool stays clean.
/// See also: SkgnodesInMemory (the persistent, cross-request layer).
pub type SkgNodeMap = HashMap<ID, SkgNode>;
#+end_src

In =server/types/skg_memory.rs= near the =SkgnodesInMemory= struct:
(see the struct definition in step 2)


* Known limitations

** The initial view gets no pool benefit

When the user first opens a view, the pool is empty (or doesn't contain
those nodes). Every node is a cache miss. The pool helps on:
- save (most nodes are already displayed -> pool hit)
- following a link to a node already in another view
- re-opening a view of a node that is still displayed elsewhere

The deferred batch-query optimization would help the initial-view case.

** Re-rendering affected views is expensive

After a save, re-rendering every affected view calls =multi_root_view=
for each. In practice the number of open views is small (< 10). If
latency becomes an issue, consider marking affected views as stale and
re-rendering lazily (when the user switches to that buffer).


* Verification

1. =cargo build= — confirm it compiles.
2. Start TypeDB: =bash/run-servers.sh &=
3. =cargo nextest run= — all existing tests pass.
4. Manual test:
   a. Open a single-root content view. Verify buffer opens. Check
      server logs for pool registration.
   b. Save the buffer. Verify pool seeding reduces disk lookups
      (look at timing logs for =complete_viewtree=).
   c. Open a second view containing a node from the first view.
      Edit the shared node in the first view, save. Verify the
      second view is automatically updated via =updated-views=.
   d. Verify warning when editing a second skg buffer while the
      first has unsaved changes.
   e. Follow a link to a node already visible. Verify pool hit.
   f. Kill a buffer. Verify close-view message sent and pool GC.
5. Integration tests: =bash/integration-tests.sh=
